{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAvjbYGUbfBk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "t_AWj0J8boYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "RaGLAJeLbqAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "id": "t9rWcDH3brk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "\n",
        "import kss"
      ],
      "metadata": {
        "id": "Lmr54nxSbsj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKEN_COUNT = 512\n",
        "N_EPOCHS = 3\n",
        "BATCH_SIZE = 2"
      ],
      "metadata": {
        "id": "7cFDzymcbu4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "pl.seed_everything(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "lRLZeSbUbwgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "w83ZzZtfbya9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "DATA_TRAIN_PATH = '/content/drive/MyDrive/train_original.json'\n",
        "df = pd.read_json(DATA_TRAIN_PATH)\n",
        "df = df.dropna()\n",
        "len(df)#, len(val_df)"
      ],
      "metadata": {
        "id": "ti5W4kZabx-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증\n",
        "DATA_TEST_PATH = '/content/drive/MyDrive/valid_original.json'\n",
        "test_df = pd.read_json(DATA_TEST_PATH)\n",
        "test_df = test_df.dropna()\n",
        "len(test_df)"
      ],
      "metadata": {
        "id": "GVftDGupb2Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습데이터에서 트레인/테스트 분리\n",
        "train_df, val_df = train_test_split(df, test_size=0.05)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "train_df.shape, val_df.shape, test_df.shape"
      ],
      "metadata": {
        "id": "sdI_jsGNb85Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 사이즈 줄이기\n",
        "downsize = 10000\n",
        "train_df = train_df[:downsize]\n",
        "test_df = test_df[:downsize//10]\n",
        "val_df = val_df[:downsize//10]"
      ],
      "metadata": {
        "id": "6C8RZzjKb82n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    outs = []\n",
        "    for doc in data['documents']:\n",
        "        line = []\n",
        "        line.append(doc['media_name'])\n",
        "        line.append(doc['id'])\n",
        "        para = []\n",
        "        for sent in doc['text']:\n",
        "            for s in sent:\n",
        "                para.append(s['sentence'])\n",
        "        line.append(para)\n",
        "        line.append(doc['abstractive'][0])\n",
        "        line.append(doc['extractive'])\n",
        "        a = doc['extractive']\n",
        "        if a[0] == None or a[1] == None or a[2] == None:\n",
        "            continue\n",
        "        outs.append(line)\n",
        "\n",
        "    outs_df = pd.DataFrame(outs)\n",
        "    outs_df.columns = ['media', 'id', 'article_original', 'abstractive', 'extractive']\n",
        "    return outs_df"
      ],
      "metadata": {
        "id": "E5Ldzxukb8z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = preprocess_data(train_df)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "eSL6tFEGb8xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "print('===== 본    문 =====')\n",
        "for idx, str in enumerate(train_df['article_original'][i]):\n",
        "    print(idx,':',str)\n",
        "print('===== 요약정답 =====')\n",
        "print(train_df['extractive'][i])\n",
        "print('===== 추출본문 =====')\n",
        "print('1 :', train_df['article_original'][i][train_df['extractive'][i][0]])\n",
        "print('2 :', train_df['article_original'][i][train_df['extractive'][i][1]])\n",
        "print('3 :', train_df['article_original'][i][train_df['extractive'][i][2]])\n",
        "print('===== 생성본문 =====')\n",
        "print(train_df['abstractive'][i])"
      ],
      "metadata": {
        "id": "YtFGFhqKb8um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(1)"
      ],
      "metadata": {
        "id": "Ej259kaHb8sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = preprocess_data(test_df)\n",
        "test_df.head(1)"
      ],
      "metadata": {
        "id": "V9ckqL6zb8cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = preprocess_data(val_df)\n",
        "val_df.head(1)"
      ],
      "metadata": {
        "id": "--qGoaIacMVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer"
      ],
      "metadata": {
        "id": "XDmg1fvYcO5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartModel\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "model = BartModel.from_pretrained('gogamza/kobart-base-v1')"
      ],
      "metadata": {
        "id": "l_Rawn7qcM7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        data: pd.DataFrame, \n",
        "        tokenizer: PreTrainedTokenizerFast, \n",
        "        max_token_len: int = 512\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        tokenlist = []\n",
        "        for sent in data_row.article_original:\n",
        "            tokenlist.append(tokenizer(\n",
        "                text = sent,\n",
        "                add_special_tokens = True)) #, # Add '[CLS]' and '[SEP]'\n",
        "    \n",
        "        src = [] # 토크나이징 된 전체 문단\n",
        "        labels = []  # 요약문에 해당하면 1, 아니면 0으로 문장수 만큼 생성\n",
        "        segs = []  #각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
        "        clss = []  #[CLS]토큰의 포지션값을 지정\n",
        "\n",
        "        odd = 0\n",
        "        for tkns in tokenlist:\n",
        "            if odd > 1 : odd = 0\n",
        "            clss = clss + [len(src)]\n",
        "            src = src + tkns['input_ids']\n",
        "            segs = segs + [odd] * len(tkns['input_ids'])\n",
        "            if tokenlist.index(tkns) in data_row.extractive :\n",
        "                labels = labels + [1]\n",
        "            else:\n",
        "                labels = labels + [0]\n",
        "            odd += 1\n",
        "        \n",
        "            #truncation\n",
        "            if len(src) == MAX_TOKEN_COUNT:\n",
        "                break\n",
        "            elif len(src) > MAX_TOKEN_COUNT:\n",
        "                src = src[:self.max_token_len - 1] + [src[-1]]\n",
        "                segs = segs[:self.max_token_len]\n",
        "                break\n",
        "    \n",
        "        #padding\n",
        "        if len(src) < MAX_TOKEN_COUNT:\n",
        "            src = src + [0]*(self.max_token_len - len(src))\n",
        "            segs = segs + [0]*(self.max_token_len - len(segs))\n",
        "            \n",
        "        if len(clss) < MAX_TOKEN_COUNT:\n",
        "            clss = clss + [-1]*(self.max_token_len - len(clss))\n",
        "        if len(labels) < MAX_TOKEN_COUNT:\n",
        "            labels = labels + [0]*(self.max_token_len - len(labels))\n",
        "\n",
        "        return dict(\n",
        "            src = torch.tensor(src),\n",
        "            segs = torch.tensor(segs),\n",
        "            clss = torch.tensor(clss),\n",
        "            labels= torch.FloatTensor(labels)\n",
        "        )"
      ],
      "metadata": {
        "id": "UegDvvoycM4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=1, max_token_len=512):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = SummDataset(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "\n",
        "        self.test_dataset = SummDataset(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "    \n",
        "        self.val_dataset = SummDataset(\n",
        "            self.val_df,\n",
        "            self.tokenizer,\n",
        "            self.max_token_len\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
        "        )"
      ],
      "metadata": {
        "id": "uPDv5UmOcM2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_module = SummDataModule(\n",
        "  train_df,\n",
        "  test_df,  \n",
        "  val_df,\n",
        "  tokenizer,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  max_token_len=MAX_TOKEN_COUNT\n",
        ")"
      ],
      "metadata": {
        "id": "l17rQ27qcMzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, dim, max_len=5000):\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
        "                              -(math.log(10000.0) / dim)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, emb, step=None):\n",
        "        emb = emb * math.sqrt(self.dim)\n",
        "        if (step):\n",
        "            emb = emb + self.pe[:, step][:, None, :]\n",
        "\n",
        "        else:\n",
        "            emb = emb + self.pe[:, :emb.size(1)]\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n",
        "\n",
        "    def get_emb(self, emb):\n",
        "        return self.pe[:, :emb.size(1)]"
      ],
      "metadata": {
        "id": "9gxj8wz4cMxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "\n",
        "    def forward(self, iter, query, inputs, mask):\n",
        "        if (iter != 0):\n",
        "            input_norm = self.layer_norm(inputs)\n",
        "        else:\n",
        "            input_norm = inputs\n",
        "\n",
        "        mask = mask.unsqueeze(1)\n",
        "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                 mask=mask)\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)"
      ],
      "metadata": {
        "id": "q_Uf47MGcMur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtTransformerEncoder(nn.Module):\n",
        "    def __init__(self, hidden_size=768, d_ff=2048, heads=8, dropout=0.3, num_inter_layers=2): \n",
        "        super(ExtTransformerEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_inter_layers = num_inter_layers\n",
        "        self.pos_emb = PositionalEncoding(dropout, hidden_size)\n",
        "        self.transformer_inter = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(hidden_size, heads, d_ff, dropout)\n",
        "            for _ in range(num_inter_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.wo = nn.Linear(hidden_size, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, top_vecs, mask):\n",
        "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
        "\n",
        "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
        "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
        "        x = top_vecs * mask[:, :, None].float()\n",
        "        x = x + pos_emb\n",
        "\n",
        "        for i in range(self.num_inter_layers):\n",
        "            x = self.transformer_inter[i](i, x, x, ~mask) \n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        sent_scores = self.sigmoid(self.wo(x))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "\n",
        "        return sent_scores"
      ],
      "metadata": {
        "id": "NxNHHfNwcaJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def gelu(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = self.dropout_1(self.gelu(self.w_1(self.layer_norm(x))))\n",
        "        output = self.dropout_2(self.w_2(inter))\n",
        "        return output + x"
      ],
      "metadata": {
        "id": "Ae6m87OgcaHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
        "        assert model_dim % head_count == 0\n",
        "        self.dim_per_head = model_dim // head_count\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.head_count = head_count\n",
        "\n",
        "        self.linear_keys = nn.Linear(model_dim,\n",
        "                                     head_count * self.dim_per_head)\n",
        "        self.linear_values = nn.Linear(model_dim,\n",
        "                                       head_count * self.dim_per_head)\n",
        "        self.linear_query = nn.Linear(model_dim,\n",
        "                                      head_count * self.dim_per_head)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_final_linear = use_final_linear\n",
        "        if (self.use_final_linear):\n",
        "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
        "    def forward(self, key, value, query, mask=None,\n",
        "                layer_cache=None, type=None, predefined_graph_1=None):\n",
        "\n",
        "        batch_size = key.size(0)\n",
        "        dim_per_head = self.dim_per_head\n",
        "        head_count = self.head_count\n",
        "        key_len = key.size(1)\n",
        "        query_len = query.size(1)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
        "                .transpose(1, 2)\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous() \\\n",
        "                .view(batch_size, -1, head_count * dim_per_head)\n",
        "\n",
        "        # 1) Project key, value, and query.\n",
        "        if layer_cache is not None:\n",
        "            if type == \"self\":\n",
        "                query, key, value = self.linear_query(query), \\\n",
        "                                    self.linear_keys(query), \\\n",
        "                                    self.linear_values(query)\n",
        "\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "\n",
        "                if layer_cache is not None:\n",
        "                    device = key.device\n",
        "                    if layer_cache[\"self_keys\"] is not None:\n",
        "                        key = torch.cat(\n",
        "                            (layer_cache[\"self_keys\"].to(device), key),\n",
        "                            dim=2)\n",
        "                    if layer_cache[\"self_values\"] is not None:\n",
        "                        value = torch.cat(\n",
        "                            (layer_cache[\"self_values\"].to(device), value),\n",
        "                            dim=2)\n",
        "                    layer_cache[\"self_keys\"] = key\n",
        "                    layer_cache[\"self_values\"] = value\n",
        "            elif type == \"context\":\n",
        "                query = self.linear_query(query)\n",
        "                if layer_cache is not None:\n",
        "                    if layer_cache[\"memory_keys\"] is None:\n",
        "                        key, value = self.linear_keys(key), \\\n",
        "                                     self.linear_values(value)\n",
        "                        key = shape(key)\n",
        "                        value = shape(value)\n",
        "                    else:\n",
        "                        key, value = layer_cache[\"memory_keys\"], \\\n",
        "                                     layer_cache[\"memory_values\"]\n",
        "                    layer_cache[\"memory_keys\"] = key\n",
        "                    layer_cache[\"memory_values\"] = value\n",
        "                else:\n",
        "                    key, value = self.linear_keys(key), \\\n",
        "                                 self.linear_values(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "        else:\n",
        "            key = self.linear_keys(key)\n",
        "            value = self.linear_values(value)\n",
        "            query = self.linear_query(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "\n",
        "        query = shape(query)\n",
        "\n",
        "        key_len = key.size(2)\n",
        "        query_len = query.size(2)\n",
        "\n",
        "        # 2) Calculate and scale scores.\n",
        "        query = query / math.sqrt(dim_per_head)\n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            scores = scores.masked_fill(mask, -1e18) # how can i fix it to use fp16...\n",
        "\n",
        "        # 3) Apply attention dropout and compute context vectors.\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "\n",
        "        if (not predefined_graph_1 is None):\n",
        "            attn_masked = attn[:, -1] * predefined_graph_1\n",
        "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
        "\n",
        "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
        "\n",
        "        drop_attn = self.dropout(attn)\n",
        "        if (self.use_final_linear):\n",
        "            context = unshape(torch.matmul(drop_attn, value))\n",
        "            output = self.final_linear(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(drop_attn, value)\n",
        "            return context"
      ],
      "metadata": {
        "id": "tz-vzp9HcaEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarizer(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
        "        super().__init__()\n",
        "        self.max_pos = 512\n",
        "        self.bert = BertModel.from_pretrained('gogamza/kobart-base-v1') #, return_dict=True)\n",
        "        self.ext_layer = ExtTransformerEncoder()\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.loss = nn.BCELoss(reduction='none')\n",
        "    \n",
        "        for p in self.ext_layer.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, segs, clss, labels=None): #, input_ids, attention_mask, labels=None):\n",
        "        \n",
        "        mask_src = ~(src == 0) #1 - (src == 0)\n",
        "        mask_cls = ~(clss == -1) #1 - (clss == -1)\n",
        "\n",
        "        top_vec = self.bert(src, token_type_ids=segs, attention_mask=mask_src)\n",
        "        top_vec = top_vec.last_hidden_state\n",
        "        \n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "\n",
        "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
        "        \n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss(sent_scores, labels)\n",
        "            \n",
        "            loss = (loss * mask_cls.float()).sum() / len(labels)\n",
        "        \n",
        "        return loss, sent_scores\n",
        "\n",
        "    def step(self, batch):\n",
        "\n",
        "        src = batch['src']\n",
        "        if len(batch['labels']) > 0 :\n",
        "            labels = batch['labels']\n",
        "        else:\n",
        "            labels = None\n",
        "        segs = batch['segs']\n",
        "        clss = batch['clss']\n",
        "        \n",
        "        loss, sent_scores = self(src, segs, clss, labels)    \n",
        "        \n",
        "        return loss, sent_scores, labels\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        \n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "        \n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "        \n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def acc_loss(self, outputs):\n",
        "        total_loss = 0\n",
        "        hit_cnt = 0\n",
        "        for outp in outputs:\n",
        "            labels = outp['labels'].cpu()\n",
        "            predictions, idxs = outp['predictions'].cpu().sort()\n",
        "            loss = outp['loss'].cpu()\n",
        "            for label, idx in zip(labels, idxs):\n",
        "                for i in range(1,3):\n",
        "                    if label[idx[-i-1]] == 1 : \n",
        "                        hit_cnt += 1\n",
        "\n",
        "            total_loss += loss\n",
        "            \n",
        "        avg_loss = total_loss / len(outputs)\n",
        "        acc = hit_cnt / (3*len(outputs)*len(labels))\n",
        "        \n",
        "        return acc, avg_loss\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        \n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        \n",
        "        print('acc:', acc, 'avg_loss:', avg_loss)\n",
        "        \n",
        "        self.log('avg_train_loss', avg_loss, prog_bar=True, logger=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        \n",
        "        print('val_acc:', acc, 'avg_val_loss:', avg_loss)\n",
        "        \n",
        "        self.log('avg_val_loss', avg_loss, prog_bar=True, logger=True)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        \n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        \n",
        "        print('test_acc:', acc, 'avg_test_loss:', avg_loss)\n",
        "        \n",
        "        self.log('avg_test_loss', avg_loss, prog_bar=True, logger=True)\n",
        "\n",
        "        return\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \n",
        "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "\n",
        "        steps_per_epoch=len(train_df) // BATCH_SIZE\n",
        "        total_training_steps = steps_per_epoch * N_EPOCHS\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=steps_per_epoch,\n",
        "            num_training_steps=total_training_steps\n",
        "        )\n",
        "\n",
        "        return dict(\n",
        "            optimizer=optimizer,\n",
        "            lr_scheduler=dict(\n",
        "                scheduler=scheduler,\n",
        "                interval='step'\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "ljLbv8vrcaCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()"
      ],
      "metadata": {
        "id": "uMzLKTwFcZ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "Hx2XROIacm07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\",\n",
        "    filename=\"best-checkpoint\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"avg_val_loss\",\n",
        "    mode=\"min\"\n",
        ")"
      ],
      "metadata": {
        "id": "GGQCNBU-cZ8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = TensorBoardLogger(\"lightning_logs\", name=\"kobart_Summary\")"
      ],
      "metadata": {
        "id": "kFwxl2UbcZ6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = EarlyStopping(monitor='avg_val_loss', patience=3)"
      ],
      "metadata": {
        "id": "w2vypsW3crAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback,early_stopping_callback],\n",
        "    logger=logger,\n",
        "    max_epochs=N_EPOCHS, # 2\n",
        "    gpus=1,\n",
        ")"
      ],
      "metadata": {
        "id": "rRgyoamMcq-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, data_module)"
      ],
      "metadata": {
        "id": "I7STU1dScq7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "mtKv54nedVjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = Summarizer.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path\n",
        ")\n",
        "trained_model.eval()\n",
        "trained_model.freeze()"
      ],
      "metadata": {
        "id": "8tSQEwggcq5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trained_model, '/content/drive/MyDrive/model_fulldata2.pt')"
      ],
      "metadata": {
        "id": "_Qa8Y5H-cq2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = torch.load('/content/drive/MyDrive/model_fulldata2.pt') "
      ],
      "metadata": {
        "id": "XkfbrWKudZn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-mecab-kor"
      ],
      "metadata": {
        "id": "xIUaz6_0dZlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "news = pd.read_csv(\"/content/drive/MyDrive/기사 예시.csv\")\n",
        "news.head()"
      ],
      "metadata": {
        "id": "QZk_0gE7dZiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = []\n",
        "for news_sentence in news['본문'].values:\n",
        "    sentence.append(news_sentence)"
      ],
      "metadata": {
        "id": "4TquUhW_dZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(text):\n",
        "    # 문장 분리 하고,\n",
        "    sents = kss.split_sentences(text)\n",
        "    \n",
        "    #데이터 가공하고,\n",
        "    tokenlist = []\n",
        "    for sent in sents:\n",
        "        tokenlist.append(tokenizer(\n",
        "            text = sent,\n",
        "            add_special_tokens = True)) #, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "    src = [] # 토크나이징 된 전체 문단\n",
        "    labels = []  # 요약문에 해당하면 1, 아니면 0으로 문장수 만큼 생성\n",
        "    segs = []  #각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
        "    clss = []  #[CLS]토큰의 포지션값을 지정\n",
        "\n",
        "    odd = 0\n",
        "\n",
        "    for tkns in tokenlist:\n",
        "\n",
        "        if odd > 1 : odd = 0\n",
        "        clss = clss + [len(src)]\n",
        "        src = src + tkns['input_ids']\n",
        "        segs = segs + [odd] * len(tkns['input_ids'])\n",
        "        odd += 1\n",
        "\n",
        "        #truncation\n",
        "        if len(src) == MAX_TOKEN_COUNT:\n",
        "            break\n",
        "        elif len(src) > MAX_TOKEN_COUNT:\n",
        "            src = src[:MAX_TOKEN_COUNT - 1] + [src[-1]]\n",
        "            segs = segs[:MAX_TOKEN_COUNT]\n",
        "            break\n",
        "\n",
        "    #padding\n",
        "    if len(src) < MAX_TOKEN_COUNT:\n",
        "        src = src + [0]*(MAX_TOKEN_COUNT - len(src))\n",
        "        segs = segs + [0]*(MAX_TOKEN_COUNT - len(segs))\n",
        "\n",
        "    if len(clss) < MAX_TOKEN_COUNT:\n",
        "        clss = clss + [-1]*(MAX_TOKEN_COUNT - len(clss))\n",
        "\n",
        "    return dict(\n",
        "        sents = sents, #정답 출력을 위해...\n",
        "        src = torch.tensor(src),\n",
        "        segs = torch.tensor(segs),\n",
        "        clss = torch.tensor(clss),\n",
        "    )"
      ],
      "metadata": {
        "id": "BB9v0qT2dZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_test(text):\n",
        "    data = data_process(text.replace('\\n',''))\n",
        "    \n",
        "    #trained_model에 넣어 결과값 반환\n",
        "    _, rtn = trained_model(data['src'].unsqueeze(0), data['segs'].unsqueeze(0), data['clss'].unsqueeze(0))\n",
        "    rtn = rtn.squeeze()\n",
        "    \n",
        "    # 예측 결과값을 받기 위한 프로세스\n",
        "    rtn_sort, idx = rtn.sort(descending = True)\n",
        "    \n",
        "    rtn_sort = rtn_sort.tolist()\n",
        "    idx = idx.tolist()\n",
        "\n",
        "    end_idx = rtn_sort.index(0)\n",
        "\n",
        "    rtn_sort = rtn_sort[:end_idx]\n",
        "    idx = idx[:end_idx]\n",
        "    \n",
        "    if len(idx) > 3:\n",
        "        rslt = idx[:3]\n",
        "    else:\n",
        "        rslt = idx\n",
        "        \n",
        "    summ = []\n",
        "    print(' ')\n",
        "    for i, r in enumerate(rslt):\n",
        "        summ.append(data['sents'][r])\n",
        "        print(summ[i])\n",
        "\n",
        "    return summ"
      ],
      "metadata": {
        "id": "c8qKIqKbdZbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sentence:\n",
        "  summarize_test(i)"
      ],
      "metadata": {
        "id": "q3tIc1a7dZYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}